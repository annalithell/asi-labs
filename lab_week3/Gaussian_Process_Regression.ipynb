{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Advanced Statistical Inference -- Gaussian Process for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Today, we will play with Gaussian processes. By the end of this lab, you will be able to \n",
    " \n",
    "- To sample from a Gaussian process prior distribution.\n",
    "- To implement Gaussian process inference for regression.\n",
    "- To use the above to observe samples from a Gaussian process posterior distribution.\n",
    "- To evaluate how different hyperparameter settings impact model quality.\n",
    "- To investigate different kernel functions and parameter optimisation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian processes (henceforth GPs) achieve greater flexibility over parametric models by imposing a preference bias as opposed to restrictive constraints. Although the parameterisation of GPs allows one to access a certain (infinite) set of functions, preference can be expressed using a prior over functions. This allows greater freedom in representing data dependencies, thus enabling the construction of better-suited models. In this lab, we shall cover the basic concepts of GP regression. For the sake of clarity, we shall focus on univariate data, which allows for better visualisation of the GP model. Nonetheless, the code implemented within this lab can be very easily extended to handle\n",
    "multi-dimensional inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "For this notebook, we will use a slightly different library: [JAX](https://docs.jax.dev/en/latest/index.html).\n",
    "\n",
    "JAX is a library for high-performance numerical computing and machine learning research, developed by Google as \"replacement\" for TensorFlow. \n",
    "\n",
    "For what we need to do, JAX is just Numpy with automatic differentiation: it implements all Numpy's functions and also their derivatives (and it allows to run accelerated code on GPUs and TPUs).\n",
    "For this notebook, you don't need to worry about the GPU acceleration, everything can run quickly on CPU.\n",
    "If you a running this notebook on Google Colab, you should already have it installed.\n",
    "\n",
    "The main difference is that we will use `jnp` instead of `np` (e.g. `import jax.numpy as jnp` instead of `import numpy as np`).\n",
    "\n",
    "In practice, this means that we can use the same code as before, but we will have to replace `np` with `jnp` in the code.\n",
    "\n",
    "For example, if we have a function that uses `np` like this:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "def my_function(x):\n",
    "    return np.sin(x) + np.cos(x)\n",
    "```\n",
    "We can replace `np` with `jnp` like this:\n",
    "\n",
    "```python\n",
    "import jax.numpy as jnp\n",
    "def my_function(x):\n",
    "    return jnp.sin(x) + jnp.cos(x)\n",
    "```\n",
    "\n",
    "Same for arrays: if we have a numpy array like this:\n",
    "\n",
    "```python\n",
    "np.zeros((3, 3))\n",
    "```\n",
    "We can replace it with a jax array like this:\n",
    "\n",
    "```python\n",
    "jnp.zeros((3, 3))\n",
    "```\n",
    "\n",
    "That's it! Nothing more!\n",
    "Later in the notebook, we will see how this small change in the code allows to compute gradients in an embarrassingly simple way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "colab = \"google.colab\" in str(get_ipython())\n",
    "preamble = r\"\"\"\\renewcommand{\\familydefault}{\\sfdefault} \\usepackage{FiraSans}\n",
    "            \\usepackage{sansmath} \\sansmath  \\usepackage{amsmath}\"\"\"\n",
    "rc(\"font\", **{\"family\": \"sans-serif\", \"sans-serif\": \"DejaVu Sans\"})\n",
    "rc(\"text\", **{\"usetex\": False, \"latex.preamble\": preamble})\n",
    "rc(\"figure\", **{\"dpi\": 200})\n",
    "rc(\n",
    "    \"axes\",\n",
    "    **{\"spines.right\": False, \"spines.top\": False, \"xmargin\": 0.0, \"ymargin\": 0.05}\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def plot_data(X, y, ax):\n",
    "    config = dict(edgecolor=\"black\", linewidth=1, facecolor=\"tab:blue\")\n",
    "    ax.scatter(X, y, label=\"Data points\", zorder=10, **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "For this notebook, we shall consider a one-dimensional regression problem.\n",
    "\n",
    "**Exercise:**\n",
    "Run the next cell to prepare and plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3424121)\n",
    "\n",
    "\n",
    "def make_dataset(a=-20, b=80, N=64, M=200, gap_ratio=0.4, ampl=1.6, leng=8, sn2=0.01):\n",
    "    def make_random_gap(X, gap_ratio=0.2):\n",
    "        a, b = X.min(), X.max()\n",
    "        gap_a = a + np.random.rand() * (b - a) * (1 - gap_ratio)\n",
    "        gap_b = gap_a + (b - a) * gap_ratio\n",
    "        idx = np.logical_and(gap_a < X, X < gap_b)\n",
    "        X[idx] = a + np.random.rand(idx.sum()) * (gap_a - a)\n",
    "        return X\n",
    "\n",
    "    def sample_random_function(X, ampl=1, leng=1, sn2=0.1):\n",
    "        n, x = X.shape[0], X / leng\n",
    "        sum_xx = np.sum(x * x, 1).reshape(-1, 1).repeat(n, 1)\n",
    "        D = sum_xx + sum_xx.transpose() - 2 * np.matmul(x, x.transpose())\n",
    "        C = ampl**2 * np.exp(-0.5 * D) + np.eye(n) * sn2\n",
    "        return np.random.multivariate_normal(np.zeros(n), C)\n",
    "\n",
    "    X = np.random.rand(N, 1) * (b - a) + a\n",
    "    X = make_random_gap(X, gap_ratio=0.4)\n",
    "    ind = np.argsort(X[..., 0])\n",
    "    y = sample_random_function(X, ampl=ampl, leng=leng, sn2=sn2)\n",
    "    Xt = np.linspace(a - 5, b + 5, M).reshape(-1, 1)\n",
    "    return X[ind], y[ind], Xt\n",
    "\n",
    "\n",
    "X, y, Xt = make_dataset()\n",
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    "plot_data(X, y, ax)\n",
    "ax.set_xlim(Xt.min(), Xt.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sampling from the GP Prior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that since GPs are non-parametric; we define a prior distribution over functions (models),\n",
    "specified as a multivariate Gaussian distribution $p(\\boldsymbol f) = \\mathcal{N} (\\boldsymbol\\mu, \\boldsymbol\\Sigma)$.\n",
    "\n",
    "Without loss of generality, we shall assume a zero-mean GP prior, i.e. $\\boldsymbol\\mu = \\boldsymbol 0$. The covariance\n",
    "matrix of the distribution, $\\boldsymbol\\Sigma$, may then be computed by evaluating the covariance between the\n",
    "input points. For this tutorial, we shall consider the widely used squared-exponential (RBF)\n",
    "covariance (also referred to as the kernel function), which is defined between two points as: \n",
    "\n",
    "$$\\kappa(\\boldsymbol x, \\boldsymbol x') = \\sigma_f^2 \\exp \\Big( -\\dfrac {\\|\\boldsymbol x-\\boldsymbol x'\\|^2}{2l^2} \\Big). $$\n",
    "\n",
    "This kernel is parameterised by a lengthscale parameter $l$, and variance $\\sigma_{f}^2$ . Given that the true\n",
    "function may be assumed to be corrupted with noise, we can also add a noise parameter, $\\sigma_\\mathrm{n}^2$ , to\n",
    "the diagonal entries of the resulting kernel matrix, $\\boldsymbol K_y$, such that\n",
    "\n",
    "$$\\boldsymbol K_y = \\boldsymbol K + \\sigma_{n}^2\\boldsymbol I.$$\n",
    "\n",
    "\n",
    "**Exercise:**\n",
    "Complete the `rbf_kernel()` function for computing the RBF kernel $\\boldsymbol K$ between two sets of input points. For a bit of flexibility\n",
    "the kernel parameters are saved in a dictionary and passed as first argument to the function.\n",
    "Hint: The `cdist` function can be used for evaluating the pairwise squared Euclidean distance between two sets of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdist(A, B):\n",
    "    \"\"\"\n",
    "    Compute all pairwise distances between vectors in A and B in matrix form.\n",
    "    \"\"\"\n",
    "    M = A.shape[0]\n",
    "    N = B.shape[0]\n",
    "    A_dots = (A * A).sum(axis=1).reshape((M, 1)) * jnp.ones(shape=(1, N))\n",
    "    B_dots = (B * B).sum(axis=1) * jnp.ones(shape=(M, 1))\n",
    "    D_squared = A_dots + B_dots - 2 * A.dot(B.T)\n",
    "    return D_squared\n",
    "\n",
    "\n",
    "def rbf_kernel(params, X1, X2):\n",
    "    lengthscale = params[\"lengthscale\"]\n",
    "    variance = params[\"variance\"]\n",
    "    # @@ COMPLETE @@\n",
    "    # out =\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compute the kernel matrix $\\kappa(\\boldsymbol{X}, \\boldsymbol{X})$. Plot it using `plt.imshow()`. Start with $\\sigma^2_f$ and $l$ both equal to one. Then try to change this two parameter. What do you see? Comment the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"lengthscale\": 1.0, \"variance\": 1.0}\n",
    "\n",
    "K = rbf_kernel(params, X, X)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bar = ax.imshow(K, extent=[X.min(), X.max(), X.min(), X.max()], cmap=\"cividis\")\n",
    "fig.colorbar(bar)\n",
    "ax.set_title(r\"Covariance matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to sample from the GP prior. Remember if $f$ follows a GP, ie $f \\sim \\mathcal{GP}(\\mu(\\cdot), \\kappa(\\cdot,\\cdot))$, then the function $f$ computed at points $X$ will have this prior $p(\\boldsymbol{f}) = \\mathcal{N}(\\boldsymbol\\mu, \\boldsymbol{\\Sigma})$, where $\\boldsymbol\\mu = \\mu(\\boldsymbol X)$ and $\\boldsymbol\\Sigma = \\kappa(\\boldsymbol X, \\boldsymbol X)$. \n",
    "\n",
    "Below you will find a couple of functions to plot a GP and some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp(x, mean, cov, palette=\"Greens\", **kwargs):\n",
    "    ax = kwargs.pop(\"ax\", plt.gca())\n",
    "    cmap = plt.get_cmap(palette)\n",
    "    ci = [1, 2, 3]\n",
    "    colors = (ci - np.min(ci)) / (np.max(ci) - np.min(ci) + 3) + 0.1\n",
    "    x = x.flatten()\n",
    "    ax.plot(x, mean, color=cmap(0.9), lw=4)\n",
    "    for i, c in enumerate(ci[::-1]):\n",
    "        up = mean + c * np.sqrt(np.diag(cov))\n",
    "        lo = mean - c * np.sqrt(np.diag(cov))\n",
    "        color = cmap(colors[i])\n",
    "        ax.fill_between(x, up, lo, color=color, alpha=0.95, **kwargs)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_samples(x, samples, palette=\"Greens\", **kwargs):\n",
    "    ax = kwargs.pop(\"ax\", plt.gca())\n",
    "    N = kwargs.pop(\"N\", 20)\n",
    "    cmap = plt.get_cmap(palette)\n",
    "    idx = np.random.randint(0, samples.shape[0], N)\n",
    "    ax.plot(x, samples[idx].T, color=cmap(0.5), lw=1, alpha=0.75, **kwargs)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Assuming a zero-mean prior, and using the RBF kernel constructed before, complete the following function to sample from the prior (use `np.random.multivariate_normal()` function).\n",
    "For the time being, you can initialise the kernel parameters as follows:\n",
    "\n",
    "- lengthscale = 15.0\n",
    "- variance = 2.0\n",
    "\n",
    "Note: make sure these parameters are floats (not integers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_f_prior(params, kernel_fn, Xt, N=20):\n",
    "    \"\"\"\n",
    "    Sample from the prior of a Gaussian process.\n",
    "    \n",
    "    Args:\n",
    "        params (dict): Parameters of the kernel function.\n",
    "        kernel_fn (callable): Kernel function.\n",
    "        Xt (array): Points at which to sample.\n",
    "        N (int): Number of samples to draw.\n",
    "    \"\"\"\n",
    "    # @@ COMPLETE @@\n",
    "    # mean = \n",
    "    # cov = \n",
    "    # samples =\n",
    "    return samples, mean, cov\n",
    "\n",
    "\n",
    "params = {\"lengthscale\": 15.0, \"variance\": 2.0}\n",
    "\n",
    "samples, mean, cov = sample_f_prior(params, rbf_kernel, Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=[10, 3], sharey=True)\n",
    "plot_samples(Xt, samples, ax=ax0)\n",
    "plot_gp(Xt, mean, cov, ax=ax1)\n",
    "plot_data(X, y, ax1)\n",
    "ax1.legend()\n",
    "fig.suptitle(r\"Gaussian process prior\", y=1.05, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Try to change the kernel hyperparameters and plot few samples from their corresponding GP prior. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inference with GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the prior represents our prior beliefs before observing the function values of any data points. Suppose we can now observe 3 points at random from the input data; we would expect that with this additional knowledge, the functions drawn from the updated GP distribution would be constrained to pass through these points (or at least close if corrupted with noise). The combination of the prior and the likelihood of the observed data leads to the posterior distribution over functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the lecture, you have derived the posterior distribution of the GP given data $\\{\\mathbf{X},\\mathbf{y}\\}$ (check the lecture note if you don't remeber).\n",
    "\n",
    "**Note**: As we have encountered in previous labs, matrix inversions can be both numerically troublesome and slow to compute. \n",
    "In this lab, we shall avoid computing matrix inversions directly by instead considering Cholesky decompositions for solving linear systems. \n",
    "You are encouraged to read more about Cholesky decompositions for GPs by consulting Appendix A.4 of [Gaussian Processes for Machine Learning (Rasmussen and Williams, 2005)](http://www.gaussianprocess.org/gpml/) - available online!\n",
    "The complete pseudo-code for the posterior inference procedure is provided in [Algorithm 2.1 from Chapter 2](https://gaussianprocess.org/gpml/chapters/RW.pdf#page=37) of this same book.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "Complete the following function to compute the GP posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_f_posterior(params, kernel_fn, X, y, Xt, sn2=0.01):\n",
    "    \"\"\"\n",
    "    Predict the posterior of a Gaussian process.\n",
    "    \n",
    "    Args:\n",
    "        params (dict): Parameters of the kernel function.\n",
    "        kernel_fn (callable): Kernel function.\n",
    "        X (array): Training points.\n",
    "        y (array): Training values.\n",
    "        Xt (array): Points at which to predict.\n",
    "        sn2 (float): Noise variance.\n",
    "    \"\"\"\n",
    "    \n",
    "    # @@ COMPLETE @@\n",
    "    # # Compute the covariance matrix K between training points and training points (with noise)\n",
    "    # # Compute the covariance matrix Kx between training points and test points\n",
    "    # # Compute the covariance matrix Kt between test points and test points\n",
    "    # K =  \n",
    "    # Kx =\n",
    "    # Kxx =\n",
    "    # # Compute the Cholesky decomposition of K\n",
    "    # L = \n",
    "    # # Compute K^{-1} y as L^T \\ (L \\ y) where \\ is the forward substitution (equivalent to jax.scipy.linalg.solve_triangular)\n",
    "    # # Step 1: Solve z = L \\ y \n",
    "    # z =\n",
    "    # # Step 2: Solve a = L^T \\ z\n",
    "    # a =\n",
    "    # # Compute the posterior mean\n",
    "    # fmean =\n",
    "    # # Compute K^{-1} Kx as L \\ Kx\n",
    "    # # Step 3: Solve v = L \\ Kx\n",
    "    # v =\n",
    "    # # Compute the posterior covariance \n",
    "    # fcov =\n",
    "\n",
    "    \n",
    "    # fmean = A.T @ V\n",
    "    # fcov = kernel_fn(params, Xt, Xt) - A.T @ A\n",
    "    # END-SOLUTION\n",
    "    return fmean, fcov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Assign 3 points at random from $\\boldsymbol{X}$ (and their corresponding function values) to `Xtr` and `ytr`\n",
    "respectively. For now we shall assume that all other $\\boldsymbol{X}$ values are unobserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(14)\n",
    "\n",
    "idx = np.random.permutation(len(X))[:5]\n",
    "\n",
    "# @@ COMPLETE @@\n",
    "# Xtr =\n",
    "# ytr =\n",
    "\n",
    "\n",
    "print(f\"Training points: {Xtr.flatten()}\")\n",
    "print(f\"Training values: {ytr.flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "Run the previously completed function to compute the GP posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@ COMPLETE @@\n",
    "# f_mean, f_cov =\n",
    "print(f\"Posterior mean (shape): {f_mean.shape}\")\n",
    "print(f\"Posterior covariance (shape): {f_cov.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Sample few times from the posterior (again use `np.random.multivariate_normal`) and, using the helper functions defined above, plot the samples and their distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "n_samples = 50\n",
    "# @@ COMPLETE @@\n",
    "# samples =\n",
    "print(f\"Samples shape: {samples.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=[10, 3], sharey=True)\n",
    "\n",
    "plot_samples(Xt, samples, palette=\"Oranges\", ax=ax0)\n",
    "plot_data(Xtr, ytr, ax0)\n",
    "plot_gp(Xt, f_mean, f_cov, ax=ax1, palette=\"Oranges\")\n",
    "plot_data(Xtr, ytr, ax1)\n",
    "ax1.legend()\n",
    "fig.suptitle(f\"Gaussian process posterior ($N={len(Xtr)}$)\", y=1.05, fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Try to add more points. Also plot the posterior with all the data available. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# @@ COMPLETE @@\n",
    "# Xtr, ytr =\n",
    "# f_mean, f_cov =\n",
    "# samples =\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=[10, 3], sharey=True)\n",
    "\n",
    "plot_samples(Xt, samples, palette=\"Oranges\", ax=ax0)\n",
    "plot_data(X, y, ax0)\n",
    "plot_gp(Xt, f_mean, f_cov, ax=ax1, palette=\"Oranges\")\n",
    "plot_data(X, y, ax1)\n",
    "fig.suptitle(f\"Posterior with N={len(Xtr)} training points\", y=1.05, fontsize=20)\n",
    "ax1.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Try to change the kernel parameters (lengthscale and variance). Plot the posterior for 3/4 combinations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "As a measure of model quality, you should also compute the (log) marginal likelihood of the model $p(\\boldsymbol{y}\\,|\\,\\boldsymbol{X})$.\n",
    "The log marginal likelihood is a measure of how well the model explains the data. It is defined as:\n",
    "$$\n",
    "\\log p(\\boldsymbol{y}\\,|\\,\\boldsymbol{X}) = \\log \\mathcal{N}(\\boldsymbol{y}\\,|\\,\\boldsymbol{0}, \\kappa(\\boldsymbol{X}, \\boldsymbol{X}) + \\sigma_{n}^2 I) \n",
    "$$\n",
    "Check the lecture notes for the definition of the log marginal likelihood. Again, you can use the Cholesky decomposition to compute the log marginal likelihood efficiently.\n",
    "Remember that the log determinant of a matrix can be computed from the Cholesky decomposition of the matrix (check connection between the diagonal of the Cholesky factor and the determinant of the matrix and eigenvalues).\n",
    "For this, complete the function below to compute the log marginal likelihood (disregard the decorator above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnums=(1))\n",
    "def marginal_likelihood(params, kernel_fn, X, y, sn2=0.01):\n",
    "    # @@ COMPLETE @@\n",
    "    # # Compute the covariance matrix K between training points and training points (with noise)\n",
    "    # K =\n",
    "    # # Compute the Cholesky decomposition of K\n",
    "    # L =\n",
    "    # # Compute the solve system L @ V = y (use jax.scipy.linalg.solve_triangular)\n",
    "    # V =\n",
    "    # # Compute the model-fitting term\n",
    "    # model_fit_term = \n",
    "    # # Compute the log-determinant term\n",
    "    # log_det_term =\n",
    "    # # Compute the log-likelihood\n",
    "    # ll =\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Compute the marginal likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"lengthscale\": 15.0, \"variance\": 1.0}\n",
    "\n",
    "print(marginal_likelihood(params, rbf_kernel, X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Run a grid search over a range of parameter values in order to determine which configuration yields the best result (based on the marginal). For this exercise, use the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthscales = np.logspace(-0.5, 1.5, 100)\n",
    "variances = np.logspace(-0.5, 1.5, 100)\n",
    "marginal_ll = np.empty((len(lengthscales), len(variances)))\n",
    "for i, lengthscale in enumerate(lengthscales):\n",
    "    for j, variance in enumerate(variances):\n",
    "        _params = {\"lengthscale\": lengthscale, \"variance\": variance}\n",
    "        # @@ COMPLETE @@\n",
    "        # marginal_ll[i, j] =\n",
    "\n",
    "best = np.unravel_index(marginal_ll.argmax(), marginal_ll.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "cset = ax.contourf(lengthscales, variances, marginal_ll.T, cmap=\"cividis\", levels=30)\n",
    "fig.colorbar(cset)\n",
    "cset = ax.contour(\n",
    "    lengthscales,\n",
    "    variances,\n",
    "    marginal_ll.T,\n",
    "    colors=\"k\",\n",
    "    linestyles=\"-\",\n",
    "    alpha=0.4,\n",
    "    levels=30,\n",
    ")\n",
    "ax.set_ylabel(\"Variance\")\n",
    "ax.set_xlabel(\"Lengthscale\")\n",
    "\n",
    "ax.plot(\n",
    "    lengthscales[best[0]],\n",
    "    variances[best[1]],\n",
    "    \"o\",\n",
    "    color=\"tab:blue\",\n",
    "    ms=5,\n",
    "    label=\"Best (for the marginal likelihood)\",\n",
    ")\n",
    "ax.plot(\n",
    "    params[\"lengthscale\"],\n",
    "    params[\"variance\"],\n",
    "    \"o\",\n",
    "    color=\"tab:red\",\n",
    "    ms=5,\n",
    "    label=\"Initial guess\"\n",
    ")\n",
    "ax.set_title(\"(Log-)Marginal Likelihood\")\n",
    "ax.legend()\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Kernel Functions (Optional)\n",
    "In the previous sections, we have seen how to sample from a GP prior and posterior, and how to perform inference with GPs.\n",
    "We have also seen how to compute the marginal likelihood of the model.\n",
    "\n",
    "But, so far we have focused exclusively on the RBF kernel.\n",
    "However, the choice of kernel function (along with its associated parameters) can have a significant effect on the overall Gaussian process model.\n",
    "Choosing the best kernel to fit your data is no simple task, and is a pertinent problem in many applied domains.<br>\n",
    "\n",
    "A brief discussion on this problem may be found here: [Kernel Cookbook](https://www.cs.toronto.edu/~duvenaud/cookbook/).\n",
    "\n",
    "Familiarise yourself better with these issues by implementing one or two additional kernels. \n",
    "\n",
    "For example, another popular kernel used in GP literature is the **Matérn kernel**.\n",
    "The Matérn covariance between two points separated by $d=||\\mathbf{x}-\\mathbf{z}||_2^2$ distance units is given by [Rasmussen & Williams](http://www.gaussianprocess.org/gpml/chapters/RW4.pdf) (Ch. 4).\n",
    "\n",
    "$$\n",
    "C_\\nu(d) = \\sigma^2\\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\\Bigg(\\sqrt{2\\nu}\\frac{d}{\\rho}\\Bigg)^\\nu K_\\nu\\Bigg(\\sqrt{2\\nu}\\frac{d}{\\rho}\\Bigg),\n",
    "$$\n",
    "\n",
    "where $\\Gamma$ is the Gamma function, $K_\\nu$ is the modified Bessel function of the second kind, and $\\rho$ and $\\nu$ are non-negative parameters of the covariance.\n",
    "\n",
    "In practice:\n",
    "\n",
    "- for $\\nu = 1/2$:    $\\qquad C_{1/2}(d) = \\sigma^2\\exp\\left(-\\frac{d}{\\rho}\\right)$,\n",
    "\n",
    "- for $\\nu = 3/2$:    $\\qquad C_{3/2}(d) = \\sigma^2\\left(1+\\frac{\\sqrt{3}d}{\\rho}\\right)\\exp\\left(-\\frac{\\sqrt{3}d}{\\rho}\\right)$,\n",
    "\n",
    "- for $\\nu = 5/2$:    $\\qquad C_{5/2}(d) = \\sigma^2\\left(1+\\frac{\\sqrt{5}d}{\\rho}+\\frac{5d^2}{3\\rho^2}\\right)\\exp\\left(-\\frac{\\sqrt{5}d}{\\rho}\\right)$.\n",
    "\n",
    "**Exercise:**\n",
    "Pick one/two versions of the Matérn kernel and implement them. Plot the prior from the GP with this new kernel. Run the inference task and report/plot the results. *Hint:* You can reuse the code for the RBF kernel to compute the distances and you don't need to change the inference function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Parameter Optimisation using Gradients\n",
    "\n",
    "Optimise the hyperparameters of the model by maximizing the marginal likelihood of the model. \n",
    "To do so, you compute the gradients of the objective function with respect to the parameters being optimised.\n",
    "The general formula is given below:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol\\theta}\\mathcal{L}(\\boldsymbol\\theta) = - \\frac{1}{2} \\text{Tr} \\left(\\boldsymbol K^{-1} (\\nabla_{\\boldsymbol\\theta}\\kappa)(\\boldsymbol X, \\boldsymbol X) \\right) + \\frac{1}{2} \\boldsymbol{y}^{T}\\boldsymbol K^{-1} (\\nabla_{\\boldsymbol\\theta}\\kappa)(\\boldsymbol X, \\boldsymbol X) \\boldsymbol K^{-1} \\boldsymbol{y}.\n",
    "$$\n",
    "\n",
    "To give a more concrete example, the $(\\nabla_{\\boldsymbol\\theta}\\kappa)(\\boldsymbol X, \\boldsymbol X)$ term for the lengthscale parameter in the RBF kernel is computed as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\kappa}{\\partial l}(\\boldsymbol x, \\boldsymbol x')  = \\sigma_{f}^2 \\exp \\left( -\\dfrac {(\\boldsymbol{x}-\\boldsymbol{x}')^2}{2l^2} \\right)\\left( \\dfrac {(\\boldsymbol{x}-\\boldsymbol{x}')^2}{l^3} \\right)\n",
    "$$\n",
    "\n",
    "Then, you can use the classic gradient **ascent** algorithm,\n",
    "\n",
    "$$\n",
    "\\boldsymbol\\theta_{t+1} = \\boldsymbol\\theta_{t} + \\eta (\\nabla_{\\boldsymbol\\theta}\\mathcal{L})(\\boldsymbol\\theta_{t}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"Millenial\" version (with the automatic differentiation of Jax)\n",
    "\n",
    "We don't really need to derive the gradients of the marginal likelihood w.r.t. parameters by hand. We can leverage the automatic differentiation engine in JAX! If you followed the instruction at the beginning of the notebook, all the operations we used are all differentiable. Let's take advantage of that! \n",
    "\n",
    "JAX has a pretty general automatic differentiation system.\n",
    "Generally, you can differentiate a (scalar) function with `jax.grad`. which operates on functions and returns a function. \n",
    "If you have a Python function `f` that evaluates the mathematical function $f$, then `jax.grad(f)` is a Python function that evaluates the mathematical function $\\nabla f$. \n",
    "That means `jax.grad(f)(x_i)` represents the value $\\nabla f(x_i)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return jnp.sin(x) + 0.1 * x\n",
    "\n",
    "def f_prime(x):\n",
    "    # Manually compute the derivative of f(x)\n",
    "    return jnp.cos(x) + 0.1 \n",
    "\n",
    "x = 1.\n",
    "print(f\"{f_prime(x)=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use JAX to compute the derivative of f(x)\n",
    "f_prime = jax.grad(f)  # Note that the argument of `jax.grad` is a function and it returns a function\n",
    "print(f\"{f_prime(x)=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you should have obtained the same result as before, when you computed the gradient by hand.\n",
    "Another convenient function is `jax.value_and_grad` for efficiently computing both a function's value as well as its gradient's value. This will compute both $f(x)$ and $\\nabla f(x)$ in a single pass, which is more efficient than computing them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_prime = jax.value_and_grad(f)\n",
    "x = 1.\n",
    "f_val, f_grad = f_prime(x)\n",
    "print(f\"{f_val=} (same as {f(x)=})\")\n",
    "print(f\"{f_grad=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally gradients are computed w.r.t. the first argument of the function, however differentiating with respect to standard Python containers just works, so use tuples, lists, and dicts (and arbitrary nesting) however you like.\n",
    "This is why in the beginning we put all the kernel parameters in a dictionary. \n",
    "\n",
    "This `grad` API has a direct correspondence to the excellent notation in Spivak's classic *Calculus on Manifolds* (1965), also used in Sussman and Wisdom's [*Structure and Interpretation of Classical Mechanics*](http://mitpress.mit.edu/sites/default/files/titles/content/sicm_edition_2/book.html) (2015) and their [*Functional Differential Geometry*](https://mitpress.mit.edu/books/functional-differential-geometry) (2013). Both books are open-access.\n",
    "\n",
    "Let's now implement this for our case. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Compute the gradient function of the marginal likelihood, using `jax.value_and_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@ COMPLETE @@\n",
    "# grad_marginal_likelihood =\n",
    "\n",
    "# If you want, you can use the following line to compile the function \n",
    "# This will compile the function for hardware acceleration (GPU/TPU)\n",
    "# It's not required for this lab, but for larger training loops it can be useful (on GPU/TPU can be 10x/100x faster)\n",
    "grad_marginal_likelihood = jax.jit(grad_marginal_likelihood, static_argnums=(1,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement this for our case. \n",
    "First of all, the next function implements the gradient update for the gradient **ascent** algorithm. We use the `jax.tree_map` function to apply the gradient update to all the parameters in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def gradient_update(params, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient ascent.\n",
    "    \n",
    "    Args:\n",
    "        params (dict): Parameters to be updated.\n",
    "        gradients (dict): Gradients of the parameters.\n",
    "        learning_rate (float): Learning rate for the update.\n",
    "    \"\"\"\n",
    "    updated_params = jax.tree_map(lambda p, g: p + learning_rate * g, params, gradients) \n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Complete the training loop, and also keep track of the marginal during optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"lengthscale\": 1.0, \"variance\": 1.0}  # Initial guess\n",
    "learning_rate = 0.001\n",
    "marginals = []  # Keep track of the marginal likelihoods for plotting here\n",
    "for _ in range(2000):\n",
    "    # @@ COMPLETE @@\n",
    "    # Compute the gradients and marginal likelihood\n",
    "    # value, gradients =\n",
    "    # Update the parameters\n",
    "    # params = \n",
    "    # Append the marginal likelihood to the list so we can plot it later\n",
    "    marginals.append(value)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Plot the marginal likelihood during optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[7, 3])\n",
    "ax.plot(marginals)\n",
    "\n",
    "ax.set_title(\"Optimization of the marginal likelihood\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Marginal Likelihood\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Finally, with these parameters, plot the predictive posterior as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# @@ COMPLETE @@\n",
    "# f_mean, f_cov =\n",
    "# samples =\n",
    "\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=[10, 3], sharey=True)\n",
    "\n",
    "plot_samples(Xt, samples, palette=\"Oranges\", ax=ax0, N=40)\n",
    "plot_data(X, y, ax0)\n",
    "\n",
    "plot_gp(Xt, f_mean, f_cov, ax=ax1, palette=\"Oranges\")\n",
    "plot_data(X, y, ax1)\n",
    "ax1.legend()\n",
    "fig.suptitle(r\"Samples from the GP posterior (with optim. kernel)\", y=1.04, fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pro tip:** Note that the RBF kernel parameters $l$ and $\\sigma_\\mathrm{f}^2$ are always expected to be positive. It is possible that the optimisation algorithm attempts to evaluate the marginal likelihood in regions of the parameter space where one or more of these parameters are negative, leading to numerical issues. \n",
    "A commonly-used technique to enforce this condition is to work with a transformed version of covariance parameters using the logarithm transformation. In particular, define $\\psi_l = \\log(l)$ and $\\psi_{f} = \\log(\\sigma_{f}^2 )$ and optimise with respect to the $\\Psi$ parameters. \n",
    "The optimisation problem in the transformed space is now unbounded, and the gradient of the likelihood should be computed with respect to the $\\Psi$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
